# ğŸ“„ RAG Pipeline â€“ Document Question-Answering API

Upload documents (PDF / DOCX / TXT), ask questions, and get **cited answers**.  
Built with **FastAPI**, **Sentence-Transformers**, **FAISS**, and **Docker**.

---

## 1 ğŸš€ Setup & Installation

### 1.1 Prerequisites
- Python **3.11+**
- Git *(optional)*
- Docker & Docker Compose **v2+** *(optional)*

---

### 1.2 Local Installation
```bash
git clone https://github.com/<YOUR_USERNAME>/rag-pipeline-assignment.git
cd rag-pipeline-assignment
pip install -r app/requirements.txt
python app/main.py
# API â†’ http://localhost:8000/docs
```

---

### 1.3 Docker Usage
```bash
docker compose build    # first build â‰ˆ 15 min
docker compose up
# API â†’ http://localhost:8000/docs
```

---

## 2 ğŸ“‹ API Usage & Testing

| Method | Path       | Purpose                                                |
|--------|-----------|--------------------------------------------------------|
| **POST** | `/upload`   | Upload â‰¤ 20 docs (â‰¤ 10 MB each, â‰¤ 1000 pages)          |
| **POST** | `/query`    | Ask a question over uploaded docs                     |
| **GET**  | `/metadata` | List stored docs & chunk counts                       |
| **GET**  | `/health`   | Service health-check                                  |

---

### Example Workflow

**ğŸ“¤ Upload**
```bash
curl -F "files=@sample.pdf" http://localhost:8000/upload
```

**â“ Query**
```bash
curl -X POST http://localhost:8000/query -H "Content-Type: application/json" -d '{"query":"What is football?","top_k":3}'
```

**ğŸ“Š Check metadata / health**
```bash
curl http://localhost:8000/metadata
curl http://localhost:8000/health
```

---

## 3 âš™ï¸ Configuring Different LLM Providers

### 3.1 Current Defaults
- **Embedding model**: `all-MiniLM-L6-v2` (Sentence-Transformers)
- **Answer generator**: Simple extractive method

---

### 3.2 ğŸ”„ Switch Embedding Model
Edit `app/utils.py` (`DocumentProcessor.init`):
```python
self.embedding_model = SentenceTransformer("intfloat/e5-base-v2")
```

---

### 3.3 ğŸ¤– Integrate OpenAI (Generative Answers)

**Install & set API key**
```bash
pip install openai
export OPENAI_API_KEY=sk-...
```

**Edit** `generate_response` in `app/utils.py`:
```python
from openai import OpenAI
client = OpenAI()

async def generate_response(self, query, chunks):
    context = "\n\n".join(c["text"] for c in chunks[:5])
    prompt = f"Answer based only on this context:\n{context}\n\nQ: {query}\nA:"
    r = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=500,
        temperature=0.1
    )
    return r.choices[0].message.content
```

---

### 3.4 ğŸŒ Other Providers

| Provider      | Install                  | Env Vars                                          |
|---------------|--------------------------|---------------------------------------------------|
| Azure OpenAI  | `pip install openai`     | `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`   |
| Cohere        | `pip install cohere`     | `COHERE_API_KEY`                                  |
| HF Inference  | `pip install huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN`                   |

Replace the OpenAI call with the providerâ€™s SDK; retrieval (FAISS) stays unchanged.

---

### 3.5 ğŸ“ `.env` Example
```env
OPENAI_API_KEY=sk-...
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
COHERE_API_KEY=
ENVIRONMENT=production
HF_ENDPOINT=https://hf-mirror.com
```

---

## 4 ğŸ— Project Structure
```
rag-pipeline/
â”œâ”€ app/
â”‚  â”œâ”€ main.py        # FastAPI entry
â”‚  â”œâ”€ utils.py       # DocumentProcessor & VectorStore
â”‚  â”œâ”€ models.py      # Pydantic schemas
â”‚  â”œâ”€ database.py    # Metadata store
â”‚  â””â”€ requirements.txt
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ .dockerignore
â””â”€ README.md
```

---

## 5 ğŸ§ª Development & Testing
```bash
pip install pytest pytest-asyncio httpx
pytest tests/
```

**ğŸ’¡ Optimization tips**:
- Use `.dockerignore`
- Cache Python dependencies
- Consider **GPU embedding** for speed

---

## 6 ğŸ¤ Contributing / Support
Open issues or PRs on [GitHub](https://github.com/<YOUR_USERNAME>/rag-pipeline-assignment) â€“ all contributions welcome!

---

Â© 2025 **RAG Pipeline Project**


## ğŸš€ How to Use Your Live RAG Pipeline Demo (Complete Guide)

Your RAG Pipeline is now live and ready for testing! Here's a comprehensive guide to explore all its features:

### ğŸŒ Access Your Live Demo
Visit your deployed application at:  
[**https://rag-pipeline-yoj9.onrender.com/docs**](https://rag-pipeline-yoj9.onrender.com/docs)  

This opens the interactive **FastAPI documentation (Swagger UI)** where you can test all endpoints directly from your browser.

---

### ğŸ“‹ Step-by-Step Usage Guide

#### **Step 1: âœ… Verify Service Health**
1. Click on **GET /health**
2. Click **Try it out**
3. Click **Execute**
4. **Expected Response**:
```json
{"status": "healthy", "service": "RAG Pipeline"}
```

---

#### **Step 2: ğŸ“¤ Upload Your Documents**
1. Click on **POST /upload**
2. Click **Try it out**
3. Click **Choose Files** and select up to **20 documents**
4. **Supported formats**: PDF, DOCX, TXT (â‰¤ 10MB each)
5. Click **Execute**
6. **Expected Response**: Success message with processed document details

---

#### **Step 3: â“ Query Your Documents**
1. Click on **POST /query**
2. Click **Try it out**
3. Enter your query in JSON format:
```json
{
  "query": "What is the main topic of the uploaded documents?",
  "top_k": 3
}
```
4. Click **Execute**
5. **Expected Response**: Intelligent answer with source citations

---

#### **Step 4: ğŸ“Š Check System Metadata (Optional)**
1. Click on **GET /metadata**
2. Click **Try it out**
3. Click **Execute**
4. **View**: Stored documents count, total chunks, and system information
